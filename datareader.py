import os
import json
from pathlib import Path
from typing import Iterator, List, Tuple, Optional

class DataReader:
    def __init__(self, folder="data", lower_case=True):
        self.folder = Path(folder)
        if not self.folder.exists() or not self.folder.is_dir():
            raise FileNotFoundError(f"Folder {self.folder} does not exist or is not a directory.")

        self.lower_case = lower_case
        self._data = []
        self._load_files()

    def _load_files(self):
        # Collect all .json files in folder (sorted for consistency)
        files = sorted(self.folder.glob("*.json"))
        if not files:
            raise FileNotFoundError(f"No JSON files found in {self.folder}")

        for file in files:
            try:
                with open(file, "r", encoding="utf-8") as f:
                    content_list = json.load(f)
                    if not isinstance(content_list, list):
                        raise ValueError(f"File {file} does not contain a JSON list.")
                    for content in content_list:
                        data = content["chat"]
                        if self.lower_case:
                            data = data.lower()
                        self._data.append(data)
            except Exception as e:
                raise RuntimeError(f"Error reading {file}: {e}")

    def __getitem__(self, index):
        # Let Python naturally raise IndexError if out of bounds
        return self._data[index]

    def __len__(self):
        return len(self._data)

    def __iter__(self):
        return iter(self._data)

    def __repr__(self):
        return f"<DataReader: {len(self)} entries from '{self.folder}'>"



class PagedDataReader:
    """
    Memory-efficient reader for a folder of JSON files where each file contains a list of
    objects with a "chat" field. Only a sliding window (cache) of N items is kept in memory.

    - cache_size: maximum number of entries to keep in memory at once (default 1000)
    - Assumes each JSON file is a top-level list (e.g., [{...}, {...}, ...])

    Access pattern optimizations:
      * __getitem__ loads the window containing the requested index
      * __iter__ streams data block-by-block to avoid holding all items

    Housekeeping:
      * Builds a lightweight index of files with their list lengths
      * Maps global indices -> (file_idx, local_idx) using cumulative counts
    """

    def __init__(self, folder: str = "data", cache_size: int = 1000, lower_case=True):
        self.folder = Path(folder)
        if not self.folder.exists() or not self.folder.is_dir():
            raise FileNotFoundError(f"Folder {self.folder} does not exist or is not a directory.")
        if cache_size <= 0:
            raise ValueError("cache_size must be > 0")

        self.cache_size = cache_size
        self.lower_case = lower_case

        # file_index: List of (path, length, cum_start)
        self._file_index: List[Tuple[Path, int, int]] = []
        self._total_len: Optional[int] = None

        # Sliding cache state
        self._cache_start: int = 0          # global index of first item in cache
        self._cache: List[str] = []         # cached chats only

        self._build_file_index()

    # --------------------- Housekeeping & indexing ---------------------
    def _build_file_index(self):
        files = sorted(self.folder.glob("*.json"))
        if not files:
            raise FileNotFoundError(f"No JSON files found in {self.folder}")

        cum = 0
        self._file_index.clear()
        for fp in files:
            length = self._count_file_items(fp)
            if length == 0:
                continue
            self._file_index.append((fp, length, cum))
            cum += length
        self._total_len = cum
        if self._total_len == 0:
            raise ValueError("All JSON files are empty (0 items).")

    def _count_file_items(self, path: Path) -> int:
        """Return the number of list elements in a JSON file without keeping the data.
        This uses json.load (which parses into memory). For huge files, consider using
        a streaming parser like `ijson`.
        """
        try:
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
                if not isinstance(data, list):
                    raise ValueError(f"File {path} does not contain a JSON list.")
                return len(data)
        except Exception as e:
            raise RuntimeError(f"Error reading {path}: {e}")

    def __len__(self) -> int:
        return int(self._total_len or 0)

    def _locate(self, global_index: int) -> Tuple[int, int]:
        """Map a global index -> (file_idx, local_idx)."""
        if global_index < 0 or global_index >= len(self):
            raise IndexError("Index out of range")
        # Binary search over cumulative ranges
        lo, hi = 0, len(self._file_index) - 1
        while lo <= hi:
            mid = (lo + hi) // 2
            fp, length, start = self._file_index[mid]
            if global_index < start:
                hi = mid - 1
            elif global_index >= start + length:
                lo = mid + 1
            else:
                return mid, global_index - start
        # Should never get here if inputs are valid
        raise IndexError("Failed to locate index")

    # --------------------- Cache loading ---------------------
    def _load_window(self, start_index: int):
        """Load a window [start_index, start_index + cache_size) into cache.
        Spans multiple files if necessary. Stores only the `chat` strings.
        """
        start_index = max(0, min(start_index, max(0, len(self) - 1)))
        end_index = min(start_index + self.cache_size, len(self))

        self._cache = []
        self._cache_start = start_index

        # Walk indices and collect chats across files
        idx = start_index
        while idx < end_index:
            file_idx, local_idx = self._locate(idx)
            fp, length, start = self._file_index[file_idx]

            # Load this file into memory once, then slice the needed portion
            try:
                with open(fp, "r", encoding="utf-8") as f:
                    items = json.load(f)
                    # Determine how many from this file belong to the window
                    take_from_file = min(length - local_idx, end_index - idx)
                    # Append chats only
                    for j in range(local_idx, local_idx + take_from_file):
                        data = items[j]["chat"]
                        if self.lower_case:
                            data = data.lower()
                        self._cache.append(data)# raises if missing
                    idx += take_from_file
            except Exception as e:
                raise RuntimeError(f"Error loading window from {fp}: {e}")

    def _ensure_in_cache(self, index: int):
        if not (self._cache_start <= index < self._cache_start + len(self._cache)):
            # Align window to a multiple of cache_size for predictable paging
            window_start = (index // self.cache_size) * self.cache_size
            self._load_window(window_start)

    # --------------------- Public API ---------------------
    def __getitem__(self, index: int) -> str:
        self._ensure_in_cache(index)
        offset = index - self._cache_start
        return self._cache[offset]

    def __iter__(self) -> Iterator[str]:
        """Stream items block-by-block using the paging cache."""
        total = len(self)
        if total == 0:
            return
        pos = 0
        while pos < total:
            self._load_window(pos)
            for item in self._cache:
                yield item
            pos += len(self._cache)

    def __repr__(self):
        return (
            f"<PagedDataReader: {len(self)} entries from '{self.folder}', "
            f"cache_size={self.cache_size}, cache_window=[{self._cache_start},"
            f"{self._cache_start + len(self._cache) - 1 if self._cache else 'âˆ…'}]>"
        )

    # -------- Optional utilities --------
    def prefetch_next(self):
        """Prefetch the next cache window (useful for strictly linear access)."""
        next_start = self._cache_start + self.cache_size
        if next_start < len(self):
            self._load_window(next_start)

    def current_window(self) -> Tuple[int, int]:
        """Return the [start, end) global index range currently cached."""
        start = self._cache_start
        end = start + len(self._cache)
        return start, end

    def file_spans(self) -> List[Tuple[str, int, int]]:
        """Return a compact summary of file ranges: (filename, start_index, length)."""
        return [(fp.name, start, length) for fp, length, start in self._file_index]
